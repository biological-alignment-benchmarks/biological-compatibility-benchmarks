{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results for Aintelope\n",
    "Run these blocks for all tests, then scroll to the title you're interested in:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.collections as mcoll\n",
    "import matplotlib.path as mpath\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "import dateutil.parser as dparser\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from aintelope.training.lightning_trainer import DQNLightning\n",
    "from aintelope.agents.memory import ReplayBuffer\n",
    "import aintelope.agents\n",
    "from aintelope.agents import get_agent_class\n",
    "#from aintelope.agents.inference_agent import InferenceAgent\n",
    "from aintelope.environments.savanna_gym import SavannaGymEnv\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n",
    "import hydra\n",
    "from hydra.core import global_hydra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.split(os.path.split(os.getcwd())[0])[0]\n",
    "\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing training runs []\n",
      "{'timestamp': '${now:%Y%m%d%H%M%S}', 'experiment_name': 'hunger', 'experiment_dir': 'outputs/${experiment_name}_${timestamp}/', 'trainer_params': {'resume_from_checkpoint': False, 'num_workers': 4, 'max_epochs': 10, 'checkpoint': '${experiment_dir}/checkpoints/', 'device': 'cpu', 'verbose': False}, 'hparams': {'batch_size': 16, 'lr': 0.001, 'env': 'savanna-gym-v2', 'env_entry_point': 'aintelope.environments.savanna_gym:SavannaGymEnv', 'env_type': 'gym', 'model': 'dqn', 'agent_id': 'instinct_agent', 'gamma': 0.99, 'sync_rate': 10, 'replay_size': 99, 'warm_start_size': 100, 'eps_last_frame': 1000, 'eps_start': 1.0, 'eps_end': 0.01, 'episode_length': 1010, 'warm_start_steps': 100, 'log_figures_every_n_epochs': 5, 'every_n_epochs': 3, 'env_params': {'num_iters': 1000, 'map_min': 0, 'map_max': 5, 'render_mode': None, 'render_map_max': 5, 'amount_agents': 1, 'amount_grass_patches': 1, 'amount_water_holes': 0}, 'agent_params': {'target_instincts': ['smell']}}}\n",
      "{'timestamp': '${now:%Y%m%d%H%M%S}', 'experiment_name': 'hunger', 'experiment_dir': 'outputs/${experiment_name}_${timestamp}/', 'trainer_params': {'resume_from_checkpoint': False, 'num_workers': 4, 'max_epochs': 10, 'checkpoint': '${experiment_dir}/checkpoints/', 'device': 'cpu', 'verbose': False}, 'hparams': {'batch_size': 16, 'lr': 0.001, 'env': 'savanna-gym-v2', 'env_entry_point': 'aintelope.environments.savanna_gym:SavannaGymEnv', 'env_type': 'gym', 'model': 'dqn', 'agent_id': 'instinct_agent', 'gamma': 0.99, 'sync_rate': 10, 'replay_size': 99, 'warm_start_size': 100, 'eps_last_frame': 1000, 'eps_start': 1.0, 'eps_end': 0.01, 'episode_length': 1010, 'warm_start_steps': 100, 'log_figures_every_n_epochs': 5, 'every_n_epochs': 3, 'env_params': {'num_iters': 1000, 'map_min': 0, 'map_max': 5, 'render_mode': None, 'render_map_max': 5, 'amount_agents': 1, 'amount_grass_patches': 1, 'amount_water_holes': 0}, 'agent_params': {'target_instincts': []}}}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No trainings have been run! make run-training* first",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg_inst)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(cfg)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dirs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo trainings have been run! make run-training* first\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: No trainings have been run! make run-training* first"
     ]
    }
   ],
   "source": [
    "outputs_dir = root_dir+'/outputs/' \n",
    "available_records = os.listdir(outputs_dir)\n",
    "print(\"Existing training runs\", available_records)\n",
    "\n",
    "dirs = [os.path.join(outputs_dir, f) for f in available_records] # add path to each file\n",
    "dirs.sort(key=lambda x: os.path.getmtime(x))\n",
    "\n",
    "global_hydra.GlobalHydra.instance().clear()\n",
    "with initialize(version_base=None, config_path=\"../config\"):\n",
    "    #conf_dir = root_dir+'/aintelope/config/config_experiment.yaml'\n",
    "    conf_dir = 'config_experiment.yaml'\n",
    "    cfg = compose(config_name=conf_dir, overrides=[])  #OmegaConf.load(conf_dir) \n",
    "    cfg_base = compose(config_name=conf_dir, overrides=[\"hparams.agent_id=q_agent\",\"hparams.agent_params.target_instincts=[]\"])\n",
    "    cfg_inst = compose(config_name=conf_dir, overrides=[\"hparams.agent_id=instinct_agent\",\"hparams.agent_params.target_instincts=['smell']\"])\n",
    "\n",
    "print(cfg_inst)\n",
    "print(cfg)\n",
    "assert len(dirs) > 0, \"No trainings have been run! make run-training* first\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_exp_dir = dirs[0]\n",
    "print(latest_exp_dir)\n",
    "print(dparser.parse(latest_exp_dir,fuzzy=True))\n",
    "df = pd.read_csv(latest_exp_dir+\"/memory_records.csv\")\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "'''\n",
    "WIP, plot what the agent sees (needs changes to InferenceAgent etc.)\n",
    "Check action values per location. Now expected reward for moving into location, but could also be\n",
    "eating in any location, or of course mapping where the food/agents are. \n",
    "'''\n",
    "#cfg = OmegaConf.load(conf_dir_base)\n",
    "\n",
    "# load environment agent\n",
    "env = SavannaGymEnv(env_params=cfg.hparams.env_params)\n",
    "env.reset() #this is also init...\n",
    "# get the brains from memory checkpoints\n",
    "model = DQNLightning.load_from_checkpoint(latest_exp_dir+\"/checkpoints/last.ckpt\")\n",
    "# disable randomness, dropout, etc...\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "keys = ([\"agent_coords\"] + \n",
    "        [f\"grass_patch_{i}\" for i in range(env.metadata[\"amount_grass_patches\"])] + \n",
    "        [f\"water_hole_{i}\" for i in range(env.metadata[\"amount_water_holes\"])])\n",
    "StateTuple = namedtuple(\"StateTuple\", {k: np.ndarray for k in keys})\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "event_x = []\n",
    "event_y = []\n",
    "event_type = []\n",
    "food_x = []\n",
    "food_y = []\n",
    "water_x = []\n",
    "water_y = []\n",
    "for _ ,row in df.iterrows():\n",
    "    \n",
    "    state = eval(row['state'])\n",
    "    #print(state)\n",
    "    x.append(state[0][0])\n",
    "    y.append(state[0][1])\n",
    "    \n",
    "    #refactor\n",
    "    food_x.append(state[1][0])\n",
    "    food_y.append(state[1][1])\n",
    "    #food_x.append(state[2][0])\n",
    "    #food_y.append(state[2][1])    \n",
    "\n",
    "    if row['instinct_events'] != '[]':\n",
    "        event_x.append(x[-1])\n",
    "        event_y.append(y[-1])\n",
    "        event_type.append(row['instinct_events'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_df = pd.DataFrame(data={'x':x, 'y':y})\n",
    "print(agent_df.head(), len(agent_df))\n",
    "\n",
    "food_df = pd.DataFrame(data={'x':food_x, 'y':food_y})\n",
    "print(food_df.head(), len(food_df))\n",
    "\n",
    "#water_df = pd.DataFrame(data={'x':water_x, 'y':water_y})\n",
    "#print(water_df.head(), len(water_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_df = pd.DataFrame(data={'x':event_x, 'y':event_y, 'event_type':event_type})\n",
    "print(len(event_df))\n",
    "print(event_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(agent_df['x'], agent_df['y'], '.r-')\n",
    "plt.plot(food_df['x'], food_df['y'], '.g', markersize=15)\n",
    "#plt.plot(water_df['x'], water_df['y'], '.b', markersize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reward received over time\n",
    "df['reward'].plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = 'autumn' # starts yellow, goes orange, then red\n",
    "n_points = len(agent_df)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111) \n",
    "cm = plt.get_cmap(color_map)\n",
    "for i in range(10):\n",
    "    ax1.set_prop_cycle('color', cm(np.linspace(0, 1, n_points - 1, endpoint=False)))\n",
    "    for i in range(n_points - 1):\n",
    "        plt.plot(agent_df['x'][i:i+2], agent_df['y'][i:i+2])\n",
    "plt.plot(food_df['x'], food_df['y'], '.g', markersize=15)\n",
    "#plt.plot(water_df['x'], water_df['y'], '.b', markersize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valuemaps for actions\n",
    "Note, these maps are new ones and don't correlate with the above ones as we randomly regenerate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "'''\n",
    "WIP, plot what the agent sees (needs changes to InferenceAgent etc.)\n",
    "Check action values per location. Now expected reward for moving into location, but could also be\n",
    "eating in any location, or of course mapping where the food/agents are. \n",
    "'''\n",
    "#cfg = OmegaConf.load(conf_dir)\n",
    "\n",
    "# load environment agent\n",
    "env = SavannaGymEnv(env_params=cfg.hparams.env_params)\n",
    "env.reset() #this is also init...\n",
    "# get the brains from memory checkpoints\n",
    "model = DQNLightning.load_from_checkpoint(latest_exp_dir+\"/checkpoints/last.ckpt\")\n",
    "# disable randomness, dropout, etc...\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the agent into each square and ask for its values for each action, then add that direction into the map\n",
    "valuemap = np.zeros((env.metadata['map_max']+2,env.metadata['map_max']+2,4))\n",
    "agent = env.agents[0]\n",
    "\n",
    "ACTION_MAP = np.array([[0, 1], [1, 0], [0, -1], [-1, 0]]) # This is a copy from savanna.py, should be an accessible param\n",
    "for x in range(0, env.metadata['map_max']):\n",
    "    for y in range(0, env.metadata['map_max']):\n",
    "        if (env.grass_patches == [x,y]).all(1).any():\n",
    "            continue\n",
    "        if (env.water_holes == [x,y]).all(1).any():\n",
    "            continue\n",
    "        env.set_agent_position(agent, np.array([x,y]))\n",
    "        observation = env.observe(agent)\n",
    "        #print(env.agent_states[agent])\n",
    "        action_vals = model(Tensor(observation)).detach().numpy()\n",
    "        offset = ACTION_MAP\n",
    "        for action in range(len(ACTION_MAP)):\n",
    "            x_ = offset[action][0]+x\n",
    "            y_ = offset[action][1]+y\n",
    "            valuemap[x_,y_,action] = action_vals[action]\n",
    "            \n",
    "valuemap = np.sum(valuemap,2)/len(ACTION_MAP)\n",
    "\n",
    "#print(valuemap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(valuemap[1:-1,1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render() isnt working atm\n",
    "maps = np.zeros((env.metadata['map_max'],env.metadata['map_max']))\n",
    "for grs in env.grass_patches:\n",
    "    print(grs[0])\n",
    "    maps[int(grs[0]),int(grs[1])] = 2.0\n",
    "for wtr in env.water_holes:\n",
    "    maps[int(wtr[0]),int(wtr[1])] = 4.0\n",
    "sns.heatmap(maps)\n",
    "# RED FOOD, LIGHT water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.set_agent_position(agent, np.array([2,2]))\n",
    "observation = env.observe(agent)\n",
    "action_vals = model(Tensor(observation)).detach().numpy()\n",
    "print(action_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model performance plots\n",
    "These plots don't have the exploration bonus as confabulators (such as epsilon-greedy).\n",
    "\n",
    "Train the model N times, then change n_latest to this N and run the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run each model for 10 different resets, with 10 different locations on the map.\n",
    "# Gather the cumulative reward, -1 has to be given on each step though\n",
    "# reset each time the food is found\n",
    "def testrun(model,env,cfg):\n",
    "    agent = get_agent_class(cfg.hparams.agent_id)(\n",
    "        env,\n",
    "        model,\n",
    "        ReplayBuffer(cfg.hparams.replay_size),\n",
    "        0,\n",
    "        cfg.hparams.agent_params,\n",
    "    )\n",
    "    epsilon = 0.0\n",
    "    device = \"cpu\"\n",
    "    start_pos = [[0,0],\n",
    "                [env.metadata['map_max'],env.metadata['map_max']],\n",
    "                [env.metadata['map_max'],0],\n",
    "                [0,env.metadata['map_max']]\n",
    "                ] #list of starting positions for agent, to test robustly each model\n",
    "    scores = []\n",
    "    rewards = []\n",
    "    for j in range(len(start_pos)):\n",
    "        agent.reset() # !!!\n",
    "        env.set_agent_position(agent, np.array(start_pos[j]))\n",
    "        for i in range(20):\n",
    "            reward, done, score = agent.play_step(model, epsilon, device) # !!!\n",
    "            rewards.append(reward) \n",
    "            scores.append(score)\n",
    "            \n",
    "    return sum(scores), sum(rewards), agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Load the models from checkpoints and run them through test envs to measure their performance \n",
    "# without training parameters (usually exploration, like epsilon). statistical significance per run\n",
    "def calc_results_per_model(test_dirs, test_cfg):\n",
    "    models = []\n",
    "    for exp_dir in test_dirs:\n",
    "        mod_dir = os.listdir(exp_dir+\"/checkpoints/\")\n",
    "        runs_dir = [os.path.join(exp_dir+\"/checkpoints/\", m) for m in mod_dir]\n",
    "        runs_dir.sort(key=lambda x: os.path.getmtime(x))\n",
    "        models.append(runs_dir)\n",
    "\n",
    "    env = SavannaGymEnv(env_params=test_cfg.hparams.env_params)\n",
    "    scores = np.zeros([len(models[0]),len(models)]) \n",
    "    rewards = np.zeros([len(models[0]),len(models)])\n",
    "    for i in range(len(models)): # different runs for statistical significance\n",
    "        for j in range(len(models[0])): # model as epochs progress\n",
    "            model = DQNLightning.load_from_checkpoint(models[i][j])\n",
    "            model.eval()\n",
    "            # run model\n",
    "            result, reward, agent = testrun(model, env, test_cfg)\n",
    "            rewards[j,i] = reward\n",
    "            scores[j,i] = result\n",
    "    return scores, rewards, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Split here for which ones in outputs are from smell and which are baseline\n",
    "base_dirs = dirs[0:1]\n",
    "inst_dirs = dirs[1:2]\n",
    "b_scores, b_rewards, b_agent = calc_results_per_model(base_dirs, cfg_base)\n",
    "i_scores, i_rewards, i_agent = calc_results_per_model(inst_dirs, cfg_inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_agent,i_agent.target_instincts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "X = np.arange(0, b_scores.shape[0]) \n",
    "\n",
    "plt.plot(X, b_rewards, color='r', label='baseline') \n",
    "plt.plot(X, i_rewards, color='g', label='instinct') \n",
    "plt.boxplot(data, labels=labels)\n",
    "plt.boxplot(data, labels=labels)\n",
    "\n",
    "plt.xlabel(\"Epoch\") \n",
    "plt.ylabel(\"Reward\") \n",
    "plt.title(\"Reward comparison\") \n",
    "plt.legend() \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add statsign boxes\n",
    "# figure out what the rewards should be from agent\n",
    "# figure out what the score should be\n",
    "# do 10 epochs\n",
    "# https://matplotlib.org/3.1.1/gallery/statistics/boxplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# fake data\n",
    "np.random.seed(19680801)\n",
    "data = np.random.lognormal(size=(37, 4), mean=1.5, sigma=1.75)\n",
    "labels = list('ABCD')\n",
    "fs = 10  # fontsize\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=3, figsize=(6, 6), sharey=True)\n",
    "axs[0, 0].boxplot(data, labels=labels)\n",
    "axs[0, 0].set_title('Default', fontsize=fs)\n",
    "\n",
    "axs[0, 1].boxplot(data, labels=labels, showmeans=True)\n",
    "axs[0, 1].set_title('showmeans=True', fontsize=fs)\n",
    "\n",
    "axs[0, 2].boxplot(data, labels=labels, showmeans=True, meanline=True)\n",
    "axs[0, 2].set_title('showmeans=True,\\nmeanline=True', fontsize=fs)\n",
    "\n",
    "axs[1, 0].boxplot(data, labels=labels, showbox=False, showcaps=False)\n",
    "tufte_title = 'Tufte Style \\n(showbox=False,\\nshowcaps=False)'\n",
    "axs[1, 0].set_title(tufte_title, fontsize=fs)\n",
    "\n",
    "axs[1, 1].boxplot(data, labels=labels, notch=True, bootstrap=10000)\n",
    "axs[1, 1].set_title('notch=True,\\nbootstrap=10000', fontsize=fs)\n",
    "\n",
    "axs[1, 2].boxplot(data, labels=labels, showfliers=False)\n",
    "axs[1, 2].set_title('showfliers=False', fontsize=fs)\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.10.3",
   "language": "python",
   "name": "python3103"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "df1d94489855e646012f575bcef68c6dcb5dcb0499f41a8ed39bb9ba537b7abb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
